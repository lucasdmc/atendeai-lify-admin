import MedicalValidationService from './sprint1-medical-validation';
import ModelEnsembleService from './sprint2-model-ensemble';
import CacheStreamingService from './sprint3-cache-streaming';
import AdvancedFeaturesService from './sprint4-advanced-features';
import { LLMOrchestratorService } from './llmOrchestratorService';

export interface AIRequest {
  message: string;
  clinicId: string;
  userId: string;
  sessionId?: string;
  context?: any;
  options?: {
    enableMedicalValidation?: boolean;
    enableEmotionAnalysis?: boolean;
    enableProactiveSuggestions?: boolean;
    enableCache?: boolean;
    enableStreaming?: boolean;
    enableMultimodal?: boolean;
    enableVoice?: boolean;
  };
}

export interface AIResponse {
  response: string;
  metadata: {
    medicalValidation?: any;
    emotionAnalysis?: any;
    proactiveSuggestions?: any[];
    cacheHit?: boolean;
    streamingMetrics?: any;
    multimodalAnalysis?: any;
    voiceResponse?: any;
    confidence: number;
    modelUsed: string;
    tokensUsed: number;
    cost: number;
    responseTime: number;
  };
}

export class AIOrchestrator {
  private static instance: AIOrchestrator;

  private constructor() {}

  public static getInstance(): AIOrchestrator {
    if (!AIOrchestrator.instance) {
      AIOrchestrator.instance = new AIOrchestrator();
    }
    return AIOrchestrator.instance;
  }

  /**
   * Processa requisi√ß√£o completa usando todos os sprints AI
   */
  async processRequest(request: AIRequest): Promise<AIResponse> {
    const startTime = Date.now();
    const options = request.options || {};
    
    try {
      // USAR O LLM ORCHESTRATOR COM MEM√ìRIA AVAN√áADA
      console.log(`ü§ñ AIOrchestrator: Processando mensagem com mem√≥ria avan√ßada`);
      
      // Extrair n√∫mero de telefone do sessionId (formato: whatsapp-554730915628)
      const phoneNumber = request.sessionId?.replace('whatsapp-', '') || request.userId;
      
      // Processar com LLM Orchestrator (sistema avan√ßado com mem√≥ria)
      const llmResponse = await LLMOrchestratorService.processMessage({
        phoneNumber,
        message: request.message,
        clinicId: request.clinicId,
        userId: request.userId
      });

      const metadata: AIResponse['metadata'] = {
        confidence: llmResponse.confidence || 0.8,
        modelUsed: llmResponse.modelUsed || 'gpt-4o',
        tokensUsed: 0,
        cost: 0,
        responseTime: Date.now() - startTime
      };

      // Sprint 1: Valida√ß√£o M√©dica (se habilitada)
      if (options.enableMedicalValidation !== false) {
        try {
          const medicalValidation = await MedicalValidationService.validateMedicalContent(
            request.message,
            request.clinicId,
            request.userId
          );
          metadata.medicalValidation = medicalValidation;

          // Se conte√∫do perigoso, bloquear resposta
          if (medicalValidation.validation_result === 'dangerous') {
            return {
              response: 'Desculpe, n√£o posso fornecer informa√ß√µes m√©dicas espec√≠ficas. Por favor, consulte um profissional de sa√∫de.',
              metadata: {
                ...metadata,
                medicalValidation,
                confidence: 0.9,
                responseTime: Date.now() - startTime
              }
            };
          }
        } catch (error) {
          console.error('Erro na valida√ß√£o m√©dica:', error);
        }
      }

      // Sprint 2: Ensemble de Modelos (se habilitado)
      if (options.enableEnsemble !== false) {
        try {
          const ensembleResponse = await ModelEnsembleService.processWithEnsemble(
            request.message,
            request.clinicId,
            request.userId,
            request.context
          );
          metadata.ensembleResponse = ensembleResponse;
        } catch (error) {
          console.error('Erro no ensemble de modelos:', error);
        }
      }

      // Sprint 3: Cache e Streaming (se habilitado)
      if (options.enableCache !== false) {
        try {
          const cacheResponse = await CacheStreamingService.processWithCacheAndStreaming(
            request.message,
            request.clinicId,
            request.userId,
            request.sessionId
          );
          metadata.cacheHit = cacheResponse.cacheHit;
          metadata.streamingMetrics = cacheResponse.streamingMetrics;
        } catch (error) {
          console.error('Erro no cache e streaming:', error);
        }
      }

      // Sprint 4: Recursos Avan√ßados (se habilitados)
      if (options.enableEmotionAnalysis !== false) {
        try {
          const emotionAnalysis = await AdvancedFeaturesService.analyzeEmotion(
            request.message,
            request.clinicId,
            request.userId
          );
          metadata.emotionAnalysis = emotionAnalysis;
        } catch (error) {
          console.error('Erro na an√°lise de emo√ß√µes:', error);
        }
      }

      if (options.enableProactiveSuggestions !== false) {
        try {
          const proactiveSuggestions = await AdvancedFeaturesService.generateProactiveSuggestions(
            request.userId,
            request.clinicId,
            'user_message',
            { message: request.message }
          );
          metadata.proactiveSuggestions = proactiveSuggestions;
        } catch (error) {
          console.error('Erro nas sugest√µes proativas:', error);
        }
      }

      // Retornar resposta do LLM Orchestrator (com mem√≥ria)
      return {
        response: llmResponse.response,
        metadata: {
          ...metadata,
          confidence: llmResponse.confidence || 0.8,
          modelUsed: llmResponse.modelUsed || 'gpt-4o',
          responseTime: Date.now() - startTime,
          // Adicionar informa√ß√µes de mem√≥ria
          memoryUsed: llmResponse.memoryUsed,
          userProfile: llmResponse.userProfile,
          conversationContext: llmResponse.conversationContext
        }
      };

    } catch (error) {
      console.error('Erro no AIOrchestrator:', error);
      
      // Resposta de fallback
      return {
        response: 'Ol√°! Como posso ajud√°-lo hoje?',
        metadata: {
          confidence: 0,
          modelUsed: 'fallback',
          tokensUsed: 0,
          cost: 0,
          responseTime: Date.now() - startTime,
          error: error instanceof Error ? error.message : 'Unknown error'
        }
      };
    }
  }

  /**
   * Testa conectividade das APIs
   */
  async testConnectivity(): Promise<any> {
    try {
      const results = {
        medicalValidation: false,
        modelEnsemble: false,
        cacheStreaming: false,
        advancedFeatures: false,
        llmOrchestrator: false
      };

      // Testar LLM Orchestrator
      try {
        const testResponse = await LLMOrchestratorService.processMessage({
          phoneNumber: 'test-phone',
          message: 'Teste de conectividade',
          clinicId: 'test-clinic',
          userId: 'test-user'
        });
        results.llmOrchestrator = !!testResponse.response;
      } catch (error) {
        console.error('Erro no teste do LLM Orchestrator:', error);
      }

      // Testar outros servi√ßos...
      try {
        await MedicalValidationService.validateMedicalContent('teste', 'test-clinic', 'test-user');
        results.medicalValidation = true;
      } catch (error) {
        console.error('Erro no teste de valida√ß√£o m√©dica:', error);
      }

      return results;
    } catch (error) {
      console.error('Erro no teste de conectividade:', error);
      throw error;
    }
  }

  /**
   * Obt√©m todas as estat√≠sticas
   */
  async getAllStats(clinicId?: string): Promise<any> {
    try {
      const stats = {
        medicalValidation: {},
        modelEnsemble: {},
        cacheStreaming: {},
        advancedFeatures: {},
        llmOrchestrator: {}
      };

      // Estat√≠sticas do LLM Orchestrator
      try {
        // Aqui voc√™ pode adicionar estat√≠sticas espec√≠ficas do LLM Orchestrator
        stats.llmOrchestrator = {
          totalRequests: 0,
          averageResponseTime: 0,
          memoryUsage: 'active'
        };
      } catch (error) {
        console.error('Erro ao obter estat√≠sticas do LLM Orchestrator:', error);
      }

      return stats;
    } catch (error) {
      console.error('Erro ao obter estat√≠sticas:', error);
      throw error;
    }
  }

  /**
   * Limpa dados antigos
   */
  async cleanupOldData(): Promise<any> {
    try {
      const results = {
        medicalValidation: false,
        modelEnsemble: false,
        cacheStreaming: false,
        advancedFeatures: false,
        llmOrchestrator: false
      };

      // Limpeza do LLM Orchestrator
      try {
        // Aqui voc√™ pode adicionar limpeza espec√≠fica do LLM Orchestrator
        results.llmOrchestrator = true;
      } catch (error) {
        console.error('Erro na limpeza do LLM Orchestrator:', error);
      }

      return results;
    } catch (error) {
      console.error('Erro na limpeza:', error);
      throw error;
    }
  }
}

export default AIOrchestrator; 